{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pickle\n",
    "# import scipy.io\n",
    "import logging\n",
    "import pdb\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import gpu_projection as gsp_gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "def pad_input_dict(in_dict):\n",
    "    \"\"\"\n",
    "    This function is for the case when the layers are flattened in a column and structured as\n",
    "    a dictionary. Each columns stored in the dictionary is  padded at the end with zeros and a\n",
    "    matrix is created.\n",
    "    \"\"\"\n",
    "    ni_list = [x.shape[0] for x in in_dict.values()]\n",
    "    max_rows = max(ni_list)\n",
    "\n",
    "    matrix = torch.zeros(max_rows, len(in_dict), device=device)\n",
    "\n",
    "    for ind in range(len(in_dict)):\n",
    "        matrix[:ni_list[ind],ind] = in_dict[ind]\n",
    "    return matrix, ni_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\n",
    "def unpad_output_mat(out_mat, ni_list):\n",
    "    out_dict = {}\n",
    "    for ind in range(out_mat.shape[1]):\n",
    "        out_dict[ind] = out_mat[:ni_list[ind],ind]\n",
    "    return out_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\n",
    "def checkCritical(pos_matrix, precision=1e-6):\n",
    "    max_elems = torch.max(pos_matrix, 0)[0]\n",
    "\n",
    "    ind_crit_bool = (abs(pos_matrix - max_elems) < precision)\n",
    "    crit_points = pos_matrix * ind_crit_bool\n",
    "\n",
    "    num_crit_points = torch.sum(ind_crit_bool, dim=0)\n",
    "\n",
    "    # Boolean of vector cols with non-trivial critical values\n",
    "    crit_cols = torch.where(num_crit_points.float() > 1, torch.ones(pos_matrix.shape[1], device=device), \\\n",
    "                            torch.zeros(pos_matrix.shape[1], device=device))\n",
    "    # getting non-trivial critical values\n",
    "    critval_list = max_elems[crit_cols.bool()]\n",
    "    critval_all_col = max_elems * crit_cols\n",
    "\n",
    "    return critval_list, max_elems, critval_all_col"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def gmu(p_matrix, xp_mat, mu=0, *args):\n",
    "    ni_tensor, inv_mask = args\n",
    "\n",
    "    vgmu = 0\n",
    "    gradg = 0\n",
    "    ni_tlist = ni_tensor.int()\n",
    "    \n",
    "    p_matrix = torch.abs(p_matrix)\n",
    "    glist = []\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "    # ni_tensor\n",
    "    betai = 1 / (torch.sqrt(ni_tensor) - 1)\n",
    "    xp_mat = p_matrix - (mu * betai)\n",
    "    indtp = xp_mat > 0\n",
    "    xp_mat.relu_()\n",
    "\n",
    "\n",
    "    # outputs\n",
    "    mnorm = torch.norm(xp_mat, dim=0)\n",
    "    mnorm_inf = mnorm.clone()\n",
    "    mnorm_inf[mnorm_inf == 0] = float(\"Inf\")\n",
    "    col_norm_mask = (mnorm > 0)\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "    # mat_mask =  (col_norm_mask.float().view(1,784) * torch.ones(300,1))\n",
    "    # mat_mask = (col_norm_mask.float().view(1, matrix.shape[1]) * torch.ones(matrix.shape[0], 1))\n",
    "\n",
    "    nip = torch.sum(xp_mat > 0, dim=0)  # columnwise number of values > 0\n",
    "\n",
    "    # needs the if condition mnorm> 0 (it's included)\n",
    "    # Terms in the Gradient Calculation\n",
    "    term2 = torch.pow(torch.sum(xp_mat, dim=0), 2)\n",
    "    mnorm_inv = torch.pow(mnorm_inf, -1)\n",
    "    mnorm_inv3 = torch.pow(mnorm_inf, -3)\n",
    "\n",
    "    # The column vectors with norm mnorm == 0 zero, should not contribute to the gradient sum.\n",
    "    # In the published algorithm, we only calculate gradients for condition: mnorm> 0\n",
    "    # To vectorize, we include in the matrix columns where mnorm == 0, but we manually replace\n",
    "    # the inf after divide by zero with 0, so that the grad of that column becomes 0 and\n",
    "    # doesn't contribute to the sum.\n",
    "    # mnorm_inv[torch.isinf(mnorm_inv)] = 0\n",
    "    # mnorm_inv3[torch.isinf(mnorm_inv3)] = 0\n",
    "\n",
    "    # Calculate Gradient\n",
    "    gradg_mat = torch.pow(betai, 2) * (-nip * mnorm_inv + term2 * mnorm_inv3)\n",
    "    gradg = torch.sum(gradg_mat)\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "    # vgmu calculation\n",
    "    ## When indtp is not empty (the columns whose norm are not zero)\n",
    "    xp_mat *= inv_mask \n",
    "    xp_mat[:, col_norm_mask] /= mnorm[col_norm_mask]\n",
    "\n",
    "    ### When indtp IS empty (the columns whose norm ARE zero)\n",
    "    # The Row Indices where maximum of that column occurs\n",
    "    max_elem_rows = torch.argmax(p_matrix, dim=0)[~col_norm_mask] \n",
    "    \n",
    "    xp_mat[max_elem_rows, ~col_norm_mask] = 1\n",
    "\n",
    "    # vgmu computation\n",
    "    vgmu_mat = betai * torch.sum(xp_mat, dim=0)\n",
    "    vgmu = torch.sum(vgmu_mat)\n",
    "\n",
    "    return vgmu, xp_mat, gradg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# --------------------------------------------------------------------------------------------------- #\n",
    "# ------------------------------------- groupedsparseproj ------------------------------------------- #\n",
    "# --------------------------------------------------------------------------------------------------- #\n",
    "def groupedsparseproj(in_dict, sps, precision=1e-6, linrat=0.9):\n",
    "    # sps = 0.9 ;  precision=1e-6; linrat=0.9\n",
    "    epsilon = 10e-15\n",
    "    k = 0\n",
    "    muup0 = 0\n",
    "\n",
    "    matrix, ni_list = pad_input_dict(in_dict)\n",
    "    ni_tensor = torch.tensor(ni_list, device=device, dtype=torch.float32)\n",
    "\n",
    "    # --------------- Create Mask ---------------------\n",
    "    inv_mask = torch.zeros(matrix.shape, device=device, dtype=torch.float32)\n",
    "    for i in range(matrix.shape[1]):\n",
    "        inv_mask[:ni_list[i],i] = torch.ones(ni_list[i])\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    r = matrix.shape[1]  # No of Columns\n",
    "    critmu = torch.tensor([])\n",
    "    critval_list = []\n",
    "\n",
    "    vgmu = torch.zeros(1, device=device)\n",
    "\n",
    "    # These operations were inside the loop, but doesn't need to be.\n",
    "    matrix_sign = torch.sign(matrix)\n",
    "    pos_matrix = matrix_sign * matrix\n",
    "    ni = matrix.shape[0]\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "    k = sum(np.sqrt(ni_list)/(np.sqrt(ni_list)-1))\n",
    "\n",
    "\n",
    "    # check critical values of mu where g(mu) is discontinuous, that is,\n",
    "    # where the two (or more) largest entries of x{i} are equal to one another.\n",
    "    critical_val, max_xi, cval_all_col = checkCritical(pos_matrix)\n",
    "\n",
    "    muup0 = max(max_xi * (torch.sqrt(ni_tensor) - 1))\n",
    "\n",
    "    # cval_all_col was extracted for the sole reason that we can multiply the critical\n",
    "    # values withe the appropriate column ni below. Hence, it preserves the column information\n",
    "    # of where the critical values came from.\n",
    "    critmu = cval_all_col * (torch.sqrt(ni_tensor) - 1) \n",
    "    critmu = critmu[critmu > 1e-6] # we only need the critival values here, not the zeros in col.\n",
    "\n",
    "    k = k - r * sps\n",
    "\n",
    "    # -------------------- gmu --------------------\n",
    "    xp_mat = torch.zeros([pos_matrix.shape[0], pos_matrix.shape[1]]).to(device)\n",
    "    # gmu_args = {'xp_mat':xp_mat, 'ni_tensor':ni_tensor}\n",
    "    \n",
    "    vgmu, xp_mat, gradg = gmu(pos_matrix, xp_mat, 0, ni_tensor, inv_mask)\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "    # if vgmu < k or abs(vgmu-k) < 1e-15:\n",
    "    if vgmu < k:     \n",
    "        xp_mat = matrix\n",
    "        gxpmu = vgmu\n",
    "        numiter = 0\n",
    "        return xp_mat\n",
    "    else:\n",
    "        numiter = 0\n",
    "        mulow = 0\n",
    "        glow = vgmu\n",
    "        muup = muup0\n",
    "        # Initialization on mu using 0, it seems to work best because the\n",
    "        # slope at zero is rather steep while it is gets flat for large mu\n",
    "        newmu = 0\n",
    "        gnew = glow\n",
    "        gpnew = gradg  # g'(0)\n",
    "        delta = muup - mulow\n",
    "        switch = True\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        while abs(gnew - k) > precision * r and numiter < 100:\n",
    "            oldmu = newmu\n",
    "            # % Secant method:\n",
    "            # % newmu = mulow + (k-glow)*(muup-mulow)/(gup-glow);\n",
    "\n",
    "            # % Bisection:\n",
    "            # % newmu = (muup+mulow)/2;\n",
    "            # % Newton:\n",
    "            newmu = oldmu + (k - gnew) / (gpnew + epsilon)\n",
    "\n",
    "            if (newmu >= muup) or (newmu <= mulow):  # If Newton goes out of the interval, use bisection\n",
    "                newmu = (mulow + muup) / 2\n",
    "\n",
    "            # print( 'Value of numiter: ' + str(numiter))\n",
    "            gnew, xnew, gpnew = gmu(pos_matrix, xp_mat, newmu, ni_tensor, inv_mask)\n",
    "                                \n",
    "            if gnew < k:\n",
    "                gup = gnew\n",
    "                xup = xnew\n",
    "                muup = newmu\n",
    "            else:\n",
    "                glow = gnew\n",
    "                mulow = xnew\n",
    "                mulow = newmu\n",
    "\n",
    "            # Guarantees linear convergence\n",
    "            if (muup - mulow) > linrat * delta and abs(oldmu - newmu) < (1 - linrat) * delta:\n",
    "                newmu = (mulow + muup) / 2\n",
    "                gnew, xnew, gpnew = gmu(pos_matrix, xp_mat, newmu, ni_tensor, inv_mask)\n",
    "\n",
    "                if gnew < k:\n",
    "                    gup = gnew\n",
    "                    xup = xnew\n",
    "                    muup = newmu\n",
    "                else:\n",
    "                    glow = gnew\n",
    "                    mulow = xnew\n",
    "                    mulow = newmu\n",
    "                numiter += 1\n",
    "            numiter += 1\n",
    "\n",
    "            if critmu.shape[0] != 0 and abs(mulow - muup) < abs(newmu) * precision and \\\n",
    "                    min(abs(newmu - critmu)) < precision * newmu:\n",
    "                print('The objective function is discontinuous around mu^*.')\n",
    "                xp = xnew\n",
    "                gxpmu = gnew\n",
    "        \n",
    "        # ----- While Loop Ends -----\n",
    "        try:\n",
    "            xp_mat = xnew\n",
    "        except:\n",
    "            # pdb.set_trace()\n",
    "            var_dict = {}\n",
    "            var_dict['in_dict'] = in_dict\n",
    "            var_dict['gnew'] = gnew\n",
    "            var_dict['k'] = k\n",
    "            var_dict['precision'] = precision\n",
    "            var_dict['r'] = r\n",
    "            var_dict['numiter'] = numiter\n",
    "            with open('var_dict.pickle', 'wb') as handle:\n",
    "                pickle.dump(var_dict, handle)\n",
    "\n",
    "        gxpmu = gnew\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # We need the column to column dot product between the two matrices xp_mat and pos_matrix\n",
    "    # Hence, we resort to Matrix- Multiplication and then extract the diagonal elements.\n",
    "    # This is equivalent to the above.\n",
    "    alpha = torch.diag(torch.matmul(xp_mat.T, pos_matrix))\n",
    "    xp_mat = alpha * (xp_mat * matrix_sign)\n",
    "    # -------------------------------------------\n",
    " \n",
    "    return xp_mat, ni_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def load_matrix_debug(mat_tuple, is_dict):\n",
    "    matrix_1, matrix_2, matrix_3, matrix_4 = mat_tuple\n",
    "    with open(matrix_1, \"rb\") as fpA:  # Pickling\n",
    "        matrix_1 = pickle.load(fpA)\n",
    "    with open(matrix_2, \"rb\") as fpA:  # Pickling\n",
    "        matrix_2 = pickle.load(fpA)\n",
    "    with open(matrix_3, \"rb\") as fpA:  # Pickling\n",
    "        matrix_3 = pickle.load(fpA)\n",
    "    with open(matrix_4, \"rb\") as fpA:  # Pickling\n",
    "        matrix_4 = pickle.load(fpA)\n",
    "\n",
    "    if is_dict == True:\n",
    "        matrix_1 = torch.from_numpy(matrix_1).view(-1)\n",
    "        matrix_2 = torch.from_numpy(matrix_2).view(-1)\n",
    "        matrix_3 = torch.from_numpy(matrix_3).view(-1)\n",
    "        matrix_4 = torch.from_numpy(matrix_4).view(-1)\n",
    "        matrix = {0:matrix_1, 1:matrix_2, 2:matrix_3, 3:matrix_4}\n",
    "    else:\n",
    "        matrix_1 = torch.from_numpy(matrix_1)\n",
    "        matrix_2 = torch.from_numpy(matrix_2)\n",
    "        matrix_3 = torch.from_numpy(matrix_3)\n",
    "        matrix_4 = torch.from_numpy(matrix_4)\n",
    "        matrix = {0:matrix_1, 1:matrix_2, 2:matrix_3, 3:matrix_4}\n",
    "\n",
    "    return matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "mat_tuple = (\"./matrices/matrix_1.pkl\", \"./matrices/matrix_2.pkl\", \"./matrices/matrix_3.pkl\", \\\n",
    "             \"./matrices/matrix_4.pkl\")\n",
    "in_dict = load_matrix_debug(mat_tuple, is_dict=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "[x.shape for x in in_dict.values()]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[torch.Size([500]),\n",
       " torch.Size([25000]),\n",
       " torch.Size([400000]),\n",
       " torch.Size([5000])]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "sps = 0.9 ;  precision=1e-6; linrat=0.9\n",
    "epsilon = 10e-15\n",
    "k = 0\n",
    "muup0 = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# matrix, ni_list = pad_input_dict(in_dict)\n",
    "# ni_tensor = torch.tensor(ni_list, device=device, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "matrix, ni_list = pad_input_dict(in_dict)\n",
    "ni_tensor = torch.tensor(ni_list, device=device, dtype=torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# --------------- Create Mask ---------------------\n",
    "inv_mask = torch.zeros(matrix.shape, device=device, dtype=torch.float32)\n",
    "# -------------------------------------------------"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# for i in range(matrix.shape[1]):\n",
    "#     inv_mask[:ni_list[i],i] = torch.ones(ni_list[i])\n",
    "# torch.where(matrix> 0, torch.tensor(1).to(device), torch.tensor(0).to(device))    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "xp_mat, ni_list = groupedsparseproj(in_dict, sps, precision=1e-6, linrat=0.9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "xp_mat[:,0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([400000])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "vec = xp_mat[:,0]\n",
    "gsp_gpu.sparsity(vec.reshape(vec.shape[0],-1))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "matrix = vec.detach().clone()\n",
    "ni = torch.tensor(matrix.shape[0], device=matrix.device)\n",
    "\n",
    "# Get Indices of columns with all-0 vectors.\n",
    "zero_col_ind = (abs(matrix.sum(0) - 0) < 2.22e-16).nonzero().reshape(-1)  \n",
    "spx_c = (torch.sqrt(ni) - torch.norm(matrix,1, dim=0) / torch.norm(matrix,2, dim=0)) / (torch.sqrt(ni) - 1)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "vec.nonzero()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[385]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def sparsity(matrix):\n",
    "    matrix = matrix.detach().clone()\n",
    "    ni = torch.tensor(matrix.shape[0], device=matrix.device)\n",
    "\n",
    "    # Get Indices of columns with all-0 vectors.\n",
    "    zero_col_ind = (abs(matrix.sum(0) - 0) < 2.22e-16).nonzero().reshape(-1)  \n",
    "    spx_c = (torch.sqrt(ni) - torch.norm(matrix,1, dim=0) / torch.norm(matrix,2, dim=0)) / (torch.sqrt(ni) - 1)\n",
    "\n",
    "    if len(zero_col_ind) != 0:\n",
    "        spx_c[zero_col_ind] = 1  # Sparsity = 1 if column already zero vector.\n",
    "    \n",
    "    if matrix.dim() > 1:   \n",
    "        # sps_avg =  spx_c.sum() / matrix.shape[1]\n",
    "        sps_avg = spx_c.mean()\n",
    "    elif matrix.dim() == 1:  # If not a matrix but a column vector!\n",
    "        sps_avg =  spx_c    \n",
    "    return sps_avg"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('imagenet': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "45e16559b97ae70bf8d810f94860f8482f9a4cb519f6949425ba42eb446cb034"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}