{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/riohib/explore/gsp-for-deeplearning/imagenet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models')\n",
    "\n",
    "\n",
    "from main_model import *\n",
    "import sys \n",
    "sys.path.append('/data/users2/rohib/github/testing')\n",
    "import utils_gsp.sps_tools as sps_tools\n",
    "import utils_gsp.gpu_projection as gsp_gpu\n",
    "\n",
    "import sys\n",
    "sys.path.append('./models')\n",
    "import models.resnet_torch as ResNet\n",
    "\n",
    "\n",
    "from models.finetuners import *\n",
    "from apply_gsp import GSP_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.__dict__['resnet18'](pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data = '/datasets01/imagenet_full_size/061417/'\n",
    "    arch = 'resnet50'\n",
    "    workers = 4\n",
    "    epochs = 1\n",
    "    start_epoch = 0\n",
    "    batch_size = 16\n",
    "    lr = 0.1\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    print_freq = 10\n",
    "    resume = \"/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/mask_stripped_best.pth.tar\"\n",
    "    evaluate = False\n",
    "    pretrained = False\n",
    "    world_size = -1\n",
    "    dist_url = 'tcp://224.66.41.62:23456'\n",
    "    dist_backend = 'nccl'\n",
    "    seed = None\n",
    "    gpu = None\n",
    "    multiprocessing_distributed = False\n",
    "\n",
    "args = Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.multiprocessing_distributed\n",
    "gsp_func = gsp_gpu\n",
    "sps = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_sps(model):\n",
    "    nonzero = total = 0\n",
    "    # print(f\"TYPE: {type(model)}\")\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name)\n",
    "        tensor = param.detach().clone()\n",
    "        # nz_count.append(torch.count_nonzero(tensor))\n",
    "        nz_count = torch.count_nonzero(tensor).item()\n",
    "        total_params = tensor.numel()\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "    \n",
    "    # print(f\"TOTAL: {total}\")\n",
    "    abs_sps = 100 * (total-nonzero) / total\n",
    "    return abs_sps, total, (total-nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                        'This will turn on the CUDNN deterministic setting, '\n",
    "                        'which can slow down your training considerably! '\n",
    "                        'You may see unexpected behavior when restarting '\n",
    "                        'from checkpoints.')\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                        'disable data parallelism.')\n",
    "\n",
    "    if args.dist_url == \"env://\" and args.world_size == -1:\n",
    "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    if args.multiprocessing_distributed:\n",
    "        # Since we have ngpus_per_node processes per node, the total world_size\n",
    "        # needs to be adjusted accordingly\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        # Use torch.multiprocessing.spawn to launch distributed processes: the\n",
    "        # main_worker process function\n",
    "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "    else:\n",
    "        # Simply call main_worker function        \n",
    "        sparse_model, dense_model, train_loader, optimizer, criterion = main_worker(args.gpu, ngpus_per_node, args)\n",
    "        \n",
    "    return sparse_model, dense_model, train_loader, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = torch.load(\"/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/model_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_masks_from_model(model_path = \"/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/model_best.pth.tar\"):\n",
    "    chkpt = torch.load(model_path)\n",
    "\n",
    "    for key in list(chkpt['state_dict']):\n",
    "        if 'mask' in key:\n",
    "            del chkpt['state_dict'][key]\n",
    "    \n",
    "    return chkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = strip_masks_from_model(model_path = \"/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/model_best.pth.tar\")\n",
    "torch.save(chkpt, \"/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/mask_stripped_best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'resnet50'\n",
      "Created model from PyTorch Models! \n",
      "\n",
      "In final Clause! and no DDP returning\n",
      "=> loading checkpoint '/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/mask_stripped_best.pth.tar'\n",
      "=> loaded checkpoint '/private/home/riohib/explore/gsp-for-deeplearning/imagenet/results/gsp_S80_fts90/mask_stripped_best.pth.tar' (epoch 241)\n"
     ]
    }
   ],
   "source": [
    "sparse_model, dense_model, train_loader, optimizer, criterion = get_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps_cp_from_dense = copy.deepcopy(dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_d = {}\n",
    "params_l = list()\n",
    "\n",
    "for name, params in sparse_model.named_parameters():\n",
    "    params_d[name] = params\n",
    "    params_l.append(params.data.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_l = list()\n",
    "for i, (name, params) in enumerate(sps_cp_from_dense.named_parameters()):\n",
    "    name_l.append(name)\n",
    "    # params = params_d[name]\n",
    "    params.data = params_l[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sps_cp_from_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.90632402072353, 25557032, 22977388)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abs_sps(sps_cp_from_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.90632402072353, 25557032, 22977388)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abs_sps(sparse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 25557032, 0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_abs_sps(dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### CKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /private/home/riohib/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/private/home/riohib/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/private/home/riohib/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py:62: UserWarning: Model 1 seems to have a lot of layers. Consider giving a list of layers whose features you are concerned with through the 'model1_layers' parameter. Your CPU/GPU will thank you :)\n",
      "  warn(\"Model 1 seems to have a lot of layers. \" \\\n",
      "/private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py:69: UserWarning: Model 2 seems to have a lot of layers. Consider giving a list of layers whose features you are concerned with through the 'model2_layers' parameter. Your CPU/GPU will thank you :)\n",
      "  warn(\"Model 2 seems to have a lot of layers. \" \\\n",
      "/private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py:145: UserWarning: Dataloader for Model 2 is not given. Using the same dataloader for both models.\n",
      "  warn(\"Dataloader for Model 2 is not given. Using the same dataloader for both models.\")\n",
      "| Comparing features |:   0%|          | 85/80073 [22:01<345:33:36, 15.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=5'>6</a>\u001b[0m dataloader \u001b[39m=\u001b[39m train_loader\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=7'>8</a>\u001b[0m cka \u001b[39m=\u001b[39m CKA(sps_cp_from_dense, dense_model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=8'>9</a>\u001b[0m           model1_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdense_resnet50\u001b[39m\u001b[39m\"\u001b[39m,   \u001b[39m# good idea to provide names to avoid confusion\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=9'>10</a>\u001b[0m           model2_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_resnet50\u001b[39m\u001b[39m\"\u001b[39m,   \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=10'>11</a>\u001b[0m         \u001b[39m#   model1_layers=layer_names_resnet18, # List of layers to extract features from\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=11'>12</a>\u001b[0m         \u001b[39m#   model2_layers=layer_names_resnet34, # extracts all layer features by default\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=12'>13</a>\u001b[0m           device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=14'>15</a>\u001b[0m cka\u001b[39m.\u001b[39;49mcompare(dataloader) \u001b[39m# secondary dataloader is optional\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdevfair/private/home/riohib/explore/gsp-for-deeplearning/imagenet/test.ipynb#ch0000014vscode-remote?line=16'>17</a>\u001b[0m results \u001b[39m=\u001b[39m cka\u001b[39m.\u001b[39mexport()\n",
      "File \u001b[0;32m~/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py:177\u001b[0m, in \u001b[0;36mCKA.compare\u001b[0;34m(self, dataloader1, dataloader2)\u001b[0m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=173'>174</a>\u001b[0m             L\u001b[39m.\u001b[39mfill_diagonal_(\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=174'>175</a>\u001b[0m             \u001b[39massert\u001b[39;00m K\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m L\u001b[39m.\u001b[39mshape, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature shape mistach! \u001b[39m\u001b[39m{\u001b[39;00mK\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mL\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=176'>177</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix[i, j, \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_HSIC(K, L) \u001b[39m/\u001b[39m num_batches\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=177'>178</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix[i, j, \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_HSIC(L, L) \u001b[39m/\u001b[39m num_batches\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=179'>180</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix[:, :, \u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix[:, :, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqrt() \u001b[39m*\u001b[39m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=180'>181</a>\u001b[0m                                                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhsic_matrix[:, :, \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msqrt())\n",
      "File \u001b[0;32m~/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py:127\u001b[0m, in \u001b[0;36mCKA._HSIC\u001b[0;34m(self, K, L)\u001b[0m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=120'>121</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=121'>122</a>\u001b[0m \u001b[39mComputes the unbiased estimate of HSIC metric.\u001b[39;00m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=122'>123</a>\u001b[0m \n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=123'>124</a>\u001b[0m \u001b[39mReference: https://arxiv.org/pdf/2010.15327.pdf Eq (3)\u001b[39;00m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=124'>125</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=125'>126</a>\u001b[0m N \u001b[39m=\u001b[39m K\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=126'>127</a>\u001b[0m ones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mones(N, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=127'>128</a>\u001b[0m result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtrace(K \u001b[39m@\u001b[39m L)\n\u001b[1;32m    <a href='file:///private/home/riohib/miniconda3/envs/cka/lib/python3.10/site-packages/torch_cka/cka.py?line=128'>129</a>\u001b[0m result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ((ones\u001b[39m.\u001b[39mt() \u001b[39m@\u001b[39m K \u001b[39m@\u001b[39m ones \u001b[39m@\u001b[39m ones\u001b[39m.\u001b[39mt() \u001b[39m@\u001b[39m L \u001b[39m@\u001b[39m ones) \u001b[39m/\u001b[39m ((N \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (N \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m)))\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch_cka import CKA\n",
    "# model1 = resnet50(pretrained=True)  # Or any neural network of your choice\n",
    "# dense_resnet50 = resnet50\n",
    "# sparse_resnet50 = model\n",
    "\n",
    "dataloader = train_loader\n",
    "\n",
    "cka = CKA(sps_cp_from_dense, dense_model,\n",
    "          model1_name=\"dense_resnet50\",   # good idea to provide names to avoid confusion\n",
    "          model2_name=\"sparse_resnet50\",   \n",
    "        #   model1_layers=layer_names_resnet18, # List of layers to extract features from\n",
    "        #   model2_layers=layer_names_resnet34, # extracts all layer features by default\n",
    "          device='cuda')\n",
    "\n",
    "cka.compare(dataloader) # secondary dataloader is optional\n",
    "\n",
    "results = cka.export()  # returns a dict that contains model names, layer names\n",
    "                        # and the CKA matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning with threshold: 0.0\n"
     ]
    }
   ],
   "source": [
    "model_gsp = GSP_Model(model) # Make a GSP Model\n",
    "\n",
    "sps_tools.prune_with_sps(model_gsp.model.module, sparsity = 0.8)\n",
    "masks_d, masks_l = sps_tools.get_conv_linear_mask(model_gsp.model.module)\n",
    "model_gsp.register_pre_hook_mask(masks_d) # This for forward pre hook mask registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for name, layer in model.named_parameters():\n",
    "    names.append(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(['mask' in x for x, _ in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename, epoch, is_best=False):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best, args, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint './results/gsp_S80/model_best.pth.tar'\n",
      "=> loaded checkpoint './results/gsp_S80/model_best.pth.tar' (epoch 125)\n",
      "Loaded State Dict: LR: 0.00400 |Best @acc1: 78.52799987792969\n"
     ]
    }
   ],
   "source": [
    "args.resume = './results/gsp_S80/model_best.pth.tar'\n",
    "\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        if args.gpu is None:\n",
    "            checkpoint = torch.load(args.resume)\n",
    "        else:\n",
    "            # Map model to be loaded to specified single gpu.\n",
    "            loc = 'cuda:{}'.format(args.gpu)\n",
    "            checkpoint = torch.load(args.resume, map_location=loc)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_acc1 = checkpoint['best_acc1']\n",
    "        if args.gpu is not None:\n",
    "            # best_acc1 may be from a checkpoint from a different GPU\n",
    "            best_acc1 = best_acc1.to(args.gpu)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "        print(f\"=> loaded checkpoint '{args.resume}' (epoch {checkpoint['epoch']})\")\n",
    "        print(f\"Loaded State Dict: LR: {optimizer.param_groups[0]['lr']:.5f} |\" \\\n",
    "                                f\"Best @acc1: {checkpoint['best_acc1']}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()\n",
    "print(args.gpu)\n",
    "# print(ngpus_per_node)\n",
    "print(args.multiprocessing_distributed)\n",
    "print(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp_model = GSP_Model(model)\n",
    "gsp_model.curr_epoch = 1\n",
    "gsp_model.curr_iter = 0\n",
    "gsp_model.gsp_int = 2\n",
    "gsp_model.sps = 0.95\n",
    "gsp_model.gsp_training_mode=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" ## Model SPS: {gsp_model.get_model_sps():.3f}\")\n",
    "print(f\" ## Epoch: {gsp_model.curr_epoch} | Start Epoch: {gsp_model.start_gsp_epoch} | iter: {gsp_model.curr_iter} | TMode: {gsp_model.gsp_training_mode} | gsp_int: {gsp_model.gsp_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp_model.apply_gsp_to_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, layer in model.named_modules():\n",
    "#     if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "#         print(f\"name: {name} | shape: {layer.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current Epoch: {gsp_model.curr_epoch}')\n",
    "print(f'Current iter: {gsp_model.curr_iter}')\n",
    "\n",
    "images, target = next(iter(train_loader))\n",
    "images = images.cuda(args.gpu, non_blocking=True)\n",
    "target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "output = gsp_model.model(images)\n",
    "# gsp_model.apply_gsp()\n",
    "\n",
    "\n",
    "gsp_model.curr_iter += 1\n",
    "\n",
    "print(f\" ## Epoch: {gsp_model.curr_epoch} | Start Epoch: {gsp_model.start_gsp_epoch} | iter: {gsp_model.curr_iter} | TMode: {gsp_model.gsp_training_mode} | gsp_int: {gsp_model.gsp_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sps_tools.get_abs_sps(model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def prune_with_sps(model, sparsity):\n",
    "    weight_d = {}\n",
    "    shape_list = []\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    weight_tensor = torch.empty(0, device=device)\n",
    "    for name, param in model.named_parameters(): \n",
    "        weight_tensor = torch.cat((weight_tensor, param.data.detach().flatten()))\n",
    "\n",
    "    wpct_val =  len(weight_tensor) * sparsity\n",
    "    sorted_weights, indices = torch.sort(weight_tensor.abs())\n",
    "    threshold = sorted_weights[:math.ceil(wpct_val)+1][-1]\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        tensor = p.data\n",
    "        # print(f'Pruning with threshold : {threshold} for layer {name}')\n",
    "        sparse_w = torch.where(abs(tensor) < threshold, torch.tensor(0.0, device=device), tensor)\n",
    "        p.data = sparse_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_with_sps(gsp_model.model, sparsity = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_linear_mask(model, threshold=1e-8, device=device):\n",
    "    masks = dict()\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            tensor = layer.weight.data\n",
    "            # Force Values smaller than threshold to 0\n",
    "            masked_tensor = torch.where(abs(tensor) < threshold, torch.tensor(0.0, device=device), tensor) \n",
    "            \n",
    "            mask = torch.where(abs(tensor) < threshold, torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n",
    "            masks[layer] = mask\n",
    "            layer.weight.data = masked_tensor\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = get_conv_linear_mask(gsp_model.model)\n",
    "# list(masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pre_hook(module, x):\n",
    "    module.mask.requires_grad_(False)\n",
    "    mask = module.mask\n",
    "    module.weight.data.mul_(mask.to(module.weight.get_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp_model.register_mask(masks)\n",
    "# list(masks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Current Epoch: {gsp_model.curr_epoch}')\n",
    "print(f'Current iter: {gsp_model.curr_iter}')\n",
    "\n",
    "images, target = next(iter(train_loader))\n",
    "images = images.cuda(args.gpu, non_blocking=True)\n",
    "target = target.cuda(args.gpu, non_blocking=True)\n",
    "\n",
    "output = gsp_model.model(images)\n",
    "\n",
    "gsp_model.curr_iter += 1\n",
    "\n",
    "# print(sps_tools.get_abs_sps(model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute output\n",
    "loss = criterion(output, target)\n",
    "\n",
    "\n",
    "# compute gradient and do SGD step\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" SPS Model: {sps_tools.get_abs_sps(model)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "447e618f7ad7c6bae28875dde16c4a71259824753409db5aab20e14cbfb03779"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cka')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
